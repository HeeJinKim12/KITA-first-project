{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "돌릴 때 T4GPU, 고용량RAM"
      ],
      "metadata": {
        "id": "ja308QNk05Nf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 패키지 import"
      ],
      "metadata": {
        "id": "pZ5XzmIVzo8A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ic2Z6PlRzg1t"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc\n",
        "from pandas import concat\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import log_loss,confusion_matrix,classification_report,roc_curve,auc\n",
        "\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from scipy import sparse\n",
        "%matplotlib inline\n",
        "seed = 42\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import concatenate\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers import Input\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Nadam\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense, Concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "uevSDJsgzu02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 로드"
      ],
      "metadata": {
        "id": "snkSXhAzzxXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 경로 설정\n",
        "\n",
        "products = pd.read_csv('/content/drive/MyDrive/DIMA 프로젝트 1조/EC21 dataset/products_no_china_no_token_v2.csv')"
      ],
      "metadata": {
        "id": "-4_BA7_zzvkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 코랩에서\n",
        "\n",
        "pip install dill"
      ],
      "metadata": {
        "id": "hMwdhmd_z3UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "# import 해야하는 모듈\n",
        "import dill\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import re"
      ],
      "metadata": {
        "id": "6ZpuGQLTz8Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전처리 함수\n",
        "- 피클 파일 없으면 함수 실행하고 저장 -> 로드 하기\n",
        "- 피클 파일 있으면 바로 로드 하기"
      ],
      "metadata": {
        "id": "rpvtJnoA0DLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(df, col_names=['CATALOG_NM', 'CATALOG_DESC']):\n",
        "\n",
        "    # html태그 확인\n",
        "    def has_html_tags(col):\n",
        "        soup = BeautifulSoup(col, 'html.parser')\n",
        "        if bool(soup.find()) :\n",
        "            return soup.get_text()\n",
        "        else:\n",
        "            return col\n",
        "\n",
        "    remove_punct_dict = {ord(punct):' ' for punct in string.punctuation}\n",
        "    lemmar = WordNetLemmatizer()\n",
        "\n",
        "    # 문장 입력받음 -> 소문자로 변환 -> 구두점 처리 -> 단어로 토큰 -> 불용어 제거 -> 어근 추출\n",
        "    def len_normalize(text):\n",
        "        tokens = nltk.word_tokenize(text.lower().translate(remove_punct_dict))\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "        return [lemmar.lemmatize(token) for token in tokens]\n",
        "\n",
        "\n",
        "    # 컬럼 선택\n",
        "    df = df[col_names]\n",
        "\n",
        "    # DESC 컬럼에 결측치 존재하는 경우\n",
        "    if df[col_names[1]].isna().sum()!=0:\n",
        "        df.fillna('',inplace=True)\n",
        "\n",
        "    # DESC 컬럼 개행문자 제거\n",
        "    df[col_names[1]] = df[col_names[1]].str.replace('\\r',' ').str.replace('\\n',' ').str.replace('\\\\',' ')\n",
        "\n",
        "    # DESC 컬럼 태그 확인 및 제거\n",
        "    df[col_names[1]] = df[col_names[1]].apply(has_html_tags)\n",
        "\n",
        "    # 태그 지우고 공백인 경우, 상품명으로 대체\n",
        "    mask = df[col_names[1]] == ''  # boolean Series 생성\n",
        "    df.loc[mask, col_names[1]] = df.loc[mask, col_names[0]]  # 조건을 만족하는 행만 선택하여 업데이트\n",
        "\n",
        "    # 불완전 태그 제거\n",
        "    pattern = re.compile(r'<[a-z]\\s+[\\d\\W\\s\\w]*\\s*=?\\s?[0-9]*[\\d\\W\\s\\w]')\n",
        "    df[col_names[1]] = df[col_names[1]].apply(lambda x: re.sub(pattern, ' ', x))\n",
        "\n",
        "    # 불완전 태그 제거한 후 공백인 경우 NM컬럼 값으로 대체\n",
        "    df[col_names[1]] = df.apply(lambda x: x[col_names[0]] if x[col_names[1]].isspace() else x[col_names[1]], axis=1)\n",
        "\n",
        "    # 클랜징\n",
        "    for i in range(len(col_names)):\n",
        "        df[col_names[i]] = df[col_names[i]].apply(len_normalize)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "z19ordGZ0AjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 피클 파일에 저장\n",
        "with open('preprocessing_function.pkl', 'wb') as file:\n",
        "    dill.dump(preprocess_text, file)"
      ],
      "metadata": {
        "id": "82YEOHZU0Md4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 함수 로드\n",
        "with open('preprocessing_function.pkl', 'rb') as file:\n",
        "    loaded_function = dill.load(file)"
      ],
      "metadata": {
        "id": "zDdiuL6x0POo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 프레임 전체 개수 줄이기"
      ],
      "metadata": {
        "id": "-H60S2480V84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_0 = products[products['judge'] == 0].sample(n=90000, random_state=42)\n",
        "df_1 = products[products['judge'] == 1].sample(n=120000, random_state=42)\n",
        "\n",
        "# Concatenate the DataFrames\n",
        "df = pd.concat([df_0, df_1])"
      ],
      "metadata": {
        "id": "UWoE7MWR0QBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df1 and y are already defined\n",
        "y = df['judge']"
      ],
      "metadata": {
        "id": "hukfHxud0lGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 피클파일 쓰는법\n",
        "- 변수 설정은 알아서"
      ],
      "metadata": {
        "id": "7Sfc_8uE2eeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = loaded_function(products, col_names=['CATALOG_NM', 'CATALOG_DESC'])"
      ],
      "metadata": {
        "id": "P7u02FYZ2dd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델\n",
        "- df['CATEGORYM_ID'] 를 바꾸기"
      ],
      "metadata": {
        "id": "IHLjQUVo0jTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Tokenization\n",
        "# 단계 1: 토큰화\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(pd.concat([df['CATALOG_DESC'], df['CATALOG_NM'],df['CATEGORYM_ID'].astype(str)]))\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "# Step 2: Integer Encoding\n",
        "sequences_desc = tokenizer.texts_to_sequences(df['CATALOG_DESC'])\n",
        "sequences_nm = tokenizer.texts_to_sequences(df['CATALOG_NM'])\n",
        "sequences_new = tokenizer.texts_to_sequences(df['CATEGORYM_ID'].astype(str))\n",
        "\n",
        "max_length_desc = max(len(seq) for seq in sequences_desc)\n",
        "max_length_nm = max(len(seq) for seq in sequences_nm)\n",
        "max_length_new = max(len(seq) for seq in sequences_new)\n",
        "\n",
        "x_desc = pad_sequences(sequences_desc, maxlen=max_length_desc, padding='post')\n",
        "x_nm = pad_sequences(sequences_nm, maxlen=max_length_nm, padding='post')\n",
        "x_new = pad_sequences(sequences_new, maxlen=max_length_new, padding='post')\n",
        "\n",
        "\n",
        "# Split the data into train and validation sets for 'NEW_INPUT'\n",
        "X_train_desc, X_test_desc, X_train_nm, X_test_nm, X_train_new, X_test_new, y_train, y_test = train_test_split(\n",
        "    x_desc, x_nm, x_new, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Split the remaining data for 'NEW_INPUT' into validation and test sets\n",
        "X_val_desc, X_test_desc, X_val_nm, X_test_nm, X_val_new, X_test_new, y_val, y_test = train_test_split(\n",
        "    X_test_desc, X_test_nm, X_test_new, y_test, test_size=0.5, random_state=40)\n",
        "\n",
        "\n",
        "# Step 2: Define LSTM model for 'CATALOG_DESC'\n",
        "model_desc = Sequential()\n",
        "model_desc.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_length_desc))\n",
        "model_desc.add(LSTM(units=100))\n",
        "model_desc.add(Dropout(0.2))\n",
        "\n",
        "# Step 3: Define LSTM model for 'CATALOG_NM'\n",
        "model_nm = Sequential()\n",
        "model_nm.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_length_nm))\n",
        "model_nm.add(LSTM(units=100))\n",
        "model_nm.add(Dropout(0.2))\n",
        "\n",
        "# Step 4: Define LSTM model for 'NEW_INPUT'\n",
        "model_new = Sequential()\n",
        "model_new.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_length_new))\n",
        "model_new.add(LSTM(units=100))\n",
        "model_new.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "# Step 5: Concatenate sequences using Keras Concatenate layer\n",
        "concat_layer = Concatenate()([model_desc.output, model_nm.output, model_new.output])\n",
        "\n",
        "# Step 6: Add additional Dense layer after concatenation if needed\n",
        "merged = Dense(units=50, activation='relu')(concat_layer)\n",
        "\n",
        "# Step 7: Output layer\n",
        "output_layer = Dense(units=1, activation='sigmoid')(merged)\n",
        "\n",
        "# Step 8: Create a new model with the concatenated layers\n",
        "model_combined = Model(inputs=[model_desc.input, model_nm.input, model_new.input], outputs=output_layer)\n",
        "\n",
        "# Step 9: Compile the model\n",
        "model_combined.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 10: Early Stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=3)\n",
        "\n",
        "# Step 11: Checkpoint\n",
        "current_directory = os.getcwd()\n",
        "models_folder_path = os.path.join(current_directory, 'Models')\n",
        "os.makedirs(models_folder_path, exist_ok=True)\n",
        "check_point = ModelCheckpoint(\n",
        "    filepath=os.path.join(models_folder_path, '{epoch:02d}-{val_accuracy:.3f}.hdf5'),\n",
        "    monitor='val_accuracy',\n",
        "    verbose=1,\n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "# Step 12: Train the model using separate validation data\n",
        "model_combined.fit(\n",
        "    [X_train_desc, X_train_nm, X_train_new],\n",
        "    y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_data=([X_val_desc, X_val_nm, X_test_new], y_val),  # Use X_test_new for validation data\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping, check_point]\n",
        ")"
      ],
      "metadata": {
        "id": "6Yb7lfOr0iPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 평가 1\n",
        "- 평가 지표\n",
        "- load_model에 쓸 모델 경로 설정해두기\n",
        "- threshold = 0.5\n"
      ],
      "metadata": {
        "id": "E_DLgFXM1GLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
        "from keras.models import load_model\n",
        "\n",
        "# load model\n",
        "loaded_model = load_model('/content/drive/MyDrive/Colab Notebooks/better.hdf5')\n",
        "\n",
        "# evaluate-score\n",
        "score = loaded_model.evaluate([X_test_desc, X_test_nm, X_test_new], y_test ,verbose=1)\n",
        "\n",
        "# Obtain predictions on the validation set\n",
        "y_pred = loaded_model.predict([X_val_desc, X_val_nm, X_val_new])\n",
        "\n",
        "# Convert probabilities to binary predictions using a threshold (e.g., 0.5)\n",
        "threshold = 0.5\n",
        "y_pred_binary = (y_pred > threshold).astype(int)\n",
        "\n",
        "# Evaluate the model\n",
        "conf_matrix = confusion_matrix(y_val, y_pred_binary)\n",
        "\n",
        "accuracy = accuracy_score(y_val, y_pred_binary)\n",
        "precision = precision_score(y_val, y_pred_binary)\n",
        "recall = recall_score(y_val, y_pred_binary)\n",
        "f1 = f1_score(y_val, y_pred_binary)\n",
        "roc_auc = roc_auc_score(y_val, y_pred)\n",
        "\n",
        "# Print the metrics\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "print(f'ROC-AUC Score: {roc_auc:.4f}')\n",
        "\n",
        "# Print the confusion matrix\n",
        "print('\\n==== Confusion Matrix ====')\n",
        "print(conf_matrix)\n",
        "\n",
        "# Print the classification report\n",
        "print('\\n==== Classification Report ====')\n",
        "print(classification_report(y_val, y_pred_binary))"
      ],
      "metadata": {
        "id": "ge7ebPEF1H96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "평가 2 maxlen 잡기"
      ],
      "metadata": {
        "id": "Q5m0IjjA3vyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('sequences_desc:' , sequences_desc)\n",
        "print('sequences_nm:' , sequences_nm)\n",
        "print('sequences_new:' , sequences_new)"
      ],
      "metadata": {
        "id": "TyTisBnP3tsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 평가 2\n",
        "- gpt 코드\n",
        "- threshold = 0.5\n",
        "- maxlen 변경하기"
      ],
      "metadata": {
        "id": "SC3xh9Bn1SQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming new_data is your DataFrame with the new data\n",
        "# Tokenize and pad the new data\n",
        "sequences_new_desc = tokenizer.texts_to_sequences(df['CATALOG_DESC'])\n",
        "sequences_new_nm = tokenizer.texts_to_sequences(df['CATALOG_NM'])\n",
        "sequences_new_new = tokenizer.texts_to_sequences(df['CATALOG_NM'])\n",
        "\n",
        "\n",
        "# Pad the sequences based on the determined maximum lengths\n",
        "x_new_desc = pad_sequences(sequences_new_desc, maxlen=5435, padding='post')\n",
        "x_new_nm = pad_sequences(sequences_new_nm, maxlen=31, padding='post')\n",
        "x_new_new = pad_sequences(sequences_new_nm, maxlen=31, padding='post')\n",
        "\n",
        "\n",
        "# Predict on the new data\n",
        "y_pred = loaded_model.predict([x_new_desc, x_new_nm, x_new_new])\n",
        "\n",
        "# Apply threshold for binary classification\n",
        "threshold = 0.5\n",
        "y_pred_binary = (y_pred > threshold).astype(int)\n",
        "\n",
        "# Get the indices corresponding to the original data\n",
        "original_indices = df.index.tolist()\n",
        "\n",
        "# Create a DataFrame with indices and corresponding predictions\n",
        "result_df = pd.DataFrame({'Original_Index': original_indices, 'Predicted_Label': y_pred_binary.flatten()})"
      ],
      "metadata": {
        "id": "fy8pS4x-2Gou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_cBQRlTl2YGH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}